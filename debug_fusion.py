#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è°ƒè¯•èåˆåŠŸèƒ½çš„ç»´åº¦é—®é¢˜
"""

import torch
from PromptAD import PromptAD

def debug_fusion_dimensions():
    """è°ƒè¯•èåˆåŠŸèƒ½çš„ç»´åº¦é—®é¢˜"""
    # è®¾ç½®åŸºæœ¬å‚æ•°
    args = {
        'dataset': 'mvtec',
        'class_name': 'carpet',
        'img_resize': 240,
        'img_cropsize': 240,
        'resolution': 400,
        'batch_size': 4,
        'k_shot': 1,
        'backbone': 'ViT-B-16-plus-240',
        'pretrained_dataset': 'laion400m_e32',
        'n_ctx': 4,
        'n_ctx_ab': 1,
        'n_pro': 1,
        'n_pro_ab': 4,
        'enable_fusion': True,
        'device': 'cuda:0' if torch.cuda.is_available() else 'cpu',
        'out_size_h': 400,
        'out_size_w': 400,
        'load_memory': False,
        'version': ''
    }
    
    print(f"ä½¿ç”¨è®¾å¤‡: {args['device']}")
    
    try:
        # åˆ›å»ºæ¨¡å‹
        print("æ­£åœ¨åˆ›å»ºPromptADæ¨¡å‹...")
        model = PromptAD(**args)
        model = model.to(args['device'])
        model.eval()
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 2
        images = torch.randn(batch_size, 3, 240, 240).to(args['device'])
        if model.precision == 'fp16':
            images = images.half()
        print(f"\næµ‹è¯•å›¾åƒå½¢çŠ¶: {images.shape}, dtype: {images.dtype}")
        
        # è·å–åŸå§‹è§†è§‰ç‰¹å¾
        print("\n=== è·å–åŸå§‹è§†è§‰ç‰¹å¾ ===")
        visual_features = model.model.encode_image(images)
        for i, feat in enumerate(visual_features):
            print(f"è§†è§‰ç‰¹å¾ {i}: {feat.shape}")
        
        # è·å–æ–‡æœ¬ç‰¹å¾
        print("\n=== è·å–æ–‡æœ¬ç‰¹å¾ ===")
        normal_text_embeddings, abnormal_text_embeddings_handle, abnormal_text_embeddings_learned = model.prompt_learner()
        print(f"normal_text_embeddings: {normal_text_embeddings.shape}")
        print(f"abnormal_text_embeddings_handle: {abnormal_text_embeddings_handle.shape}")
        print(f"abnormal_text_embeddings_learned: {abnormal_text_embeddings_learned.shape}")
        
        # ç¼–ç æ–‡æœ¬ç‰¹å¾
        normal_text_features = model.encode_text_embedding(normal_text_embeddings, model.tokenized_normal_prompts)
        abnormal_text_features_handle = model.encode_text_embedding(abnormal_text_embeddings_handle, model.tokenized_abnormal_prompts_handle)
        abnormal_text_features_learned = model.encode_text_embedding(abnormal_text_embeddings_learned, model.tokenized_abnormal_prompts_learned)
        
        print(f"normal_text_features: {normal_text_features.shape}")
        print(f"abnormal_text_features_handle: {abnormal_text_features_handle.shape}")
        print(f"abnormal_text_features_learned: {abnormal_text_features_learned.shape}")
        
        # åˆå¹¶å¼‚å¸¸æ–‡æœ¬ç‰¹å¾
        abnormal_text_features = torch.cat([abnormal_text_features_handle, abnormal_text_features_learned], dim=0)
        print(f"åˆå¹¶åçš„abnormal_text_features: {abnormal_text_features.shape}")
        
        # æµ‹è¯•ç¬¬ä¸€ä¸ªç‰¹å¾ï¼ˆpooledï¼‰çš„èåˆ
        print("\n=== æµ‹è¯•pooledç‰¹å¾èåˆ ===")
        v_feat = visual_features[0]  # pooledç‰¹å¾
        print(f"v_featå½¢çŠ¶: {v_feat.shape}")
        
        text_feat = normal_text_features.mean(dim=0, keepdim=True)
        print(f"text_featå½¢çŠ¶: {text_feat.shape}")
        
        # æ‰©å±•æ–‡æœ¬ç‰¹å¾
        if len(v_feat.shape) == 2:  # [B, D]
            text_feat = text_feat.expand(v_feat.shape[0], -1)
        print(f"æ‰©å±•åtext_featå½¢çŠ¶: {text_feat.shape}")
        
        # é€‰æ‹©èåˆæ¨¡å—
        feat_dim = v_feat.shape[-1]
        print(f"ç‰¹å¾ç»´åº¦: {feat_dim}")
        
        if feat_dim == 640:
            cross_attention = model.cross_modal_attention_640
            fusion_weight_net = model.fusion_weight_net_640
            projection = model.fused_projection_640
        elif feat_dim == 896:
            cross_attention = model.cross_modal_attention_896
            fusion_weight_net = model.fusion_weight_net_896
            projection = model.fused_projection_896
        
        print(f"æŠ•å½±å±‚è¾“å…¥ç»´åº¦: {projection.in_features}")
        print(f"æŠ•å½±å±‚è¾“å‡ºç»´åº¦: {projection.out_features}")
        
        # è·¨æ¨¡æ€æ³¨æ„åŠ›èåˆ
        print("\n=== è·¨æ¨¡æ€æ³¨æ„åŠ›èåˆ ===")
        v_input = v_feat.unsqueeze(1) if len(v_feat.shape) == 2 else v_feat
        t_input = text_feat.unsqueeze(1) if len(text_feat.shape) == 2 else text_feat
        print(f"æ³¨æ„åŠ›è¾“å…¥ - v_input: {v_input.shape}, t_input: {t_input.shape}")
        
        v_fused, t_fused = cross_attention(v_input, t_input)
        print(f"æ³¨æ„åŠ›è¾“å‡º - v_fused: {v_fused.shape}, t_fused: {t_fused.shape}")
        
        # å¤„ç†æ³¨æ„åŠ›è¾“å‡ºç»´åº¦
        if len(v_feat.shape) == 2:
            v_fused = v_fused.squeeze(1)
            t_fused = t_fused.squeeze(1)
        else:
            v_fused = v_fused.mean(1)
            t_fused = t_fused.mean(1)
        
        print(f"å¤„ç†å - v_fused: {v_fused.shape}, t_fused: {t_fused.shape}")
        
        # ç‰¹å¾æ‹¼æ¥
        combined_feat = torch.cat([v_fused, t_fused], dim=-1)
        print(f"æ‹¼æ¥åç‰¹å¾: {combined_feat.shape}")
        
        print("\nâœ… è°ƒè¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è°ƒè¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

if __name__ == '__main__':
    print("å¼€å§‹è°ƒè¯•èåˆåŠŸèƒ½ç»´åº¦...")
    success = debug_fusion_dimensions()
    if success:
        print("\nğŸ‰ è°ƒè¯•å®Œæˆï¼")
    else:
        print("\nğŸ’¥ è°ƒè¯•å¤±è´¥ã€‚")